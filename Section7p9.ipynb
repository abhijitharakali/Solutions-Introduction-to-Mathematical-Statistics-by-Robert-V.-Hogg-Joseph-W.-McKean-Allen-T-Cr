{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c05fc26",
   "metadata": {},
   "source": [
    "The solutions manual also has some solutions. I have tried to solve as many from the rest (i.e. from the ones whose solutions are not in the soluton manual) as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45f95d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import pylab as py\n",
    "import scipy.linalg as la\n",
    "import statistics\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "\n",
    "from math import gamma as tma\n",
    "import itertools\n",
    "from scipy.stats import laplace\n",
    "from scipy.stats import logistic\n",
    "from scipy.stats import cauchy\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import weibull_min as weibull\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal as mnorm\n",
    "from scipy.stats import t as studt\n",
    "from scipy.stats import f as fdist\n",
    "from scipy.stats import chisquare as chisq\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import gaussian_kde as gkde\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import math\n",
    "import sympy as sym\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.cbook import boxplot_stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049c7dd9",
   "metadata": {},
   "source": [
    "Following is directly from the text\n",
    "\n",
    "#### Theorem 7.9.1.\n",
    "\n",
    "Let $X_1, X_2, \\cdots , X_n$ denote a random sample from a distribution having a pdf $f(x;\\theta),~ \\theta \\in \\Omega,$ where $\\Omega$ is an interval set. Suppose that the statistic $Y_1$ is a complete and sufficient statistic for $\\theta$. Let $Z = u(X_1,X_2,\\cdots,X_n)$ be any other statistic (not a function of $Y_1$ alone). If the distribution of $Z$ does not depend upon $\\theta$, then $Z$ is independent of the sufficient statistic $Y_1$.\n",
    "\n",
    "The following lemma is based of the material in the beginning of Section $7.9$ of the text which is somewhat of a converse of the theorem above.\n",
    "\n",
    "#### Lemma 7.9.1.\n",
    "\n",
    "Let $X_1, X_2, \\cdots , X_n$ denote a random sample from a distribution having a pdf $f(x;\\theta),~ \\theta \\in \\Omega,$ where $\\Omega$ is an interval set. Suppose that the statistic $Y_1$ is a sufficient statistic for $\\theta$. Let $Z = u(X_1,X_2,\\cdots,X_n)$ be any other statistic. If $Z$ is independent of the sufficient statistic $Y_1$, then the distribution of $Z$ does not depend upon $\\theta$. That is, $Z$ is an ancillary statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7415ce23",
   "metadata": {},
   "source": [
    "#### Exercise 7.9.1. \n",
    "\n",
    "Let $Y_1 < Y_2 < Y_3 < Y_4$ denote the order statistics of a random sample of size $n = 4$ from a distribution having pdf $f(x;\\theta) = 1/\\theta,~ 0 < x < \\theta,$ zero elsewhere, where $0 < \\theta < \\infty.$ Argue that the complete sufficient statistic $Y_4$ for $\\theta$ is independent of each of the statistics $Y_1/Y_4$ and $(Y_1 + Y_2)/(Y_3 + Y_4).$\n",
    "\n",
    "*Hint*: Show that the pdf is of the form $(1/\\theta)f(x/\\theta)$, where $f(w) = 1,~ 0 < w < 1,$ zero elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e6ee9f",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "If $f(w) = I_{(0,1)}(w)$, then \n",
    "$$\n",
    "\\begin{align}\n",
    "f(x;\\theta) & \\triangleq \\frac{1}{\\theta} \\times I_{(0,\\theta)}(x) \\\\ \n",
    "&= \\frac{1}{\\theta} f\\left(\\frac{x}{\\theta}\\right).\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "Note that \n",
    "\n",
    "$$I_{(0,1)}\\left(\\frac{x}{\\theta}\\right) = I_{(0,\\theta)}(x).$$\n",
    "\n",
    "From example $7.8.5$ of the text, we know that if a random variable $W$ has pdf $f(w)$, then the transformation $X=\\theta W$ will have a pdf of $1/\\theta f(x/\\theta)$ which is (from above equations) same as $f(x;\\theta).$ So our random samples $X_i$ are scaled versions of random variables $W_i$ sampled from $U(0,1)$. From theorem $7.9.1$, any scale invariant statistic $Z$ of our random samples $X_i$ will be independent of the complete sufficient statistic for $\\theta$.\n",
    "\n",
    "Now both $Y_1/Y_4$ and $(Y_1 + Y_2)/(Y_3 + Y_4)$ are scale invariant and hence, they will be independent of the complete sufficient statistic $Y_4$ of $\\theta$ from theorem $7.9.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b5431",
   "metadata": {},
   "source": [
    "#### Exercise 7.9.2. \n",
    "\n",
    "Let $Y_1 < Y_2 < \\cdots < Y_n$ be the order statistics of a random sample from a $N(\\theta,\\sigma^2),~ −\\infty < \\theta < \\infty,$ distribution. Show that the distribution of $Z = Y_n − \\overline{X}$ does not depend upon $\\theta$. Thus $\\overline{Y} =\\sum_{i=1}^n Y_i/n$, a complete sufficient statistic for $\\theta$ is independent of $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa91ee69",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "The statistic $Z = u(X_1,X_2,\\cdots,X_n) = Y_n-\\overline{X}$ is location invariant, i.e.\n",
    "\n",
    "$$u(x_1,x_2,\\cdots,x_n) = u(x_1+d,x_2+d,\\cdots,x_n+d), \\forall d \\in \\mathbb{R}.$$\n",
    "\n",
    "If random samples $\\{W_i\\}_{i=1}^n$ from $N(0,\\sigma^2)$ are taken and we derive $X_i = W_i + \\theta$, then the resulting statistic $Z = u(X_1,X_2,\\cdots,X_n)$ will be equal to $u(W_1,W_2,\\cdots,W_n)$ whose distribution is independent of the location $\\theta$. Hence, from theorem $7.9.1$ $Z$ will be independent of the complete sufficient statistic $\\overline{Y}$ of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e6c0c",
   "metadata": {},
   "source": [
    "#### Exercise 7.9.3\n",
    "\n",
    "Solutions manual has the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941dd291",
   "metadata": {},
   "source": [
    "#### Exercise 7.9.4. \n",
    "\n",
    "Let $X$ and $Y$ be random variables such that $E(X^k)$ and $E(Y^k) \\neq 0$ exist for $k = 1, 2, 3, \\cdots .$ If the ratio $X/Y$ and its denominator $Y$ are independent, prove that $E[(X/Y)^k] = E(X^k)/E(Y^k),$ $k = 1,2,3,\\cdots .$\n",
    "\n",
    "*Hint*: Write $E(X^k) = E[Y^k(X/Y)^k].$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaa731a",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "Following the hint, if $X/Y$ and $Y$ are independent, then so are $(X/Y)^k$ and $Y^k$ (see https://math.stackexchange.com/a/8743/145325). So $E(X^k) = E[Y^k(X/Y)^k] = E[Y^k]E[(X/Y)^k] \\implies$ $E[(X/Y)^k] = E(X^k)/E(Y^k)$ since we are given that $E(Y^k) \\neq 0$ for $k = 1, 2, 3, \\cdots .$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545be2cd",
   "metadata": {},
   "source": [
    "#### Exercise 7.9.5\n",
    "\n",
    "Solutions manual has the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9293f4",
   "metadata": {},
   "source": [
    "#### Exercise 7.9.6\n",
    "\n",
    "Let $X_1,X_2,\\cdots,X_5$ be iid with pdf $f(x) = e^{−x}, 0 < x < \\infty,$ zero elsewhere. Show that $(X_1 + X_2)/(X_1 + X_2 + \\cdots + X_5)$ and its denominator are independent. \n",
    "\n",
    "*Hint*: The pdf $f(x)$ is a member of $\\{f(x;\\theta) : 0 < \\theta < \\infty \\},$ where $f(x;\\theta) = (1/\\theta)e^{−x}/\\theta, 0 < x < \\infty,$ zero elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95de526c",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "As given in the hint, the pdf belongs to a family that is scaled version of a standard exponential distribution. The given statistic $Z = (X_1 + X_2)/(X_1 + X_2 + \\cdots + X_5)$ is scale invariant, and the denominator $Y_1 = X_1 + X_2 + \\cdots + X_5$ is the complete sufficient statistic for this family (which belongs to the exponential family of pdfs). Hence they ($Z $ and $ Y_1$) are independent (from theorem $7.9.1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b076d18",
   "metadata": {},
   "source": [
    "#### Exercise 7.9.7\n",
    "\n",
    "Solutions manual has the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aca3ba",
   "metadata": {},
   "source": [
    "#### Exercise 7.9.8\n",
    "\n",
    "Let $Y_1 < Y_2 < \\cdots < Y_n$ be the order statistics of a random sample from a distribution with the pdf\n",
    "\n",
    "$$ f(x;\\theta_1,\\theta_2) = \\frac{1}{\\theta_2} \\exp{-\\left(\\frac{x-\\theta_1}{\\theta_2}\\right)},$$ \n",
    "\n",
    "$\\theta_1 <x<\\infty,$ zero elsewhere, where $−\\infty<\\theta_1 <\\infty, ~0<\\theta_2 <\\infty.$ Show that the joint complete sufficient statistics $Y_1$ and $\\overline{X} = \\overline{Y}$ for the parameters $\\theta_1$ and $\\theta_2$ are independent of $(Y_2 −Y_1)/\\sum_{i=1}^n(Y_i −Y_1).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464fdd5c",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "Same as solution to the previous exercise given in the solutions manual. The given statistic is location and scale invariant statistic and hence, independent of the complete and sufficient statistics $Y_1$ and $\\overline{X} = \\overline{Y}$ for the location and scale parameters $\\theta_1$ and $\\theta_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0ae13a",
   "metadata": {},
   "source": [
    "#### Exercise 7.9.9\n",
    "\n",
    "Solutions manual has the solution. However, I want to add a comment for part (b).\n",
    "\n",
    "Authors ask if the statistic $R = (X_1^2 + X_2^2)/(X_1^2 + \\cdots + X_5^2)$ has a F-distribution. This might be because in section $3.6.2$, they introduce F-distribution as a random variable that can be expressed as an (appropriately scaled) ratio of two ***independent*** Chi-squared random variables. As given in the solutions manual, the numerator and denominator are not independent in this case and hence do not have an F-distribution. So then what is the distribution of $R$?\n",
    "\n",
    "Let \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A & \\triangleq \\frac{X_1^2 + X_2^2}{\\theta} \\\\\n",
    "B & \\triangleq \\frac{X_3^2 + X_4^2+ X_5^2}{\\theta}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then $R = A/(A+B)$ where $A$ and $B$ are independent random variables having Chi-squared distribution which is a special case of Gamma distribution. Now refer to Section $3.3.2$ of the text, equation $3.3.9$. From that material, we know that $R$ has a beta distribution after we adjust for the scale.\n",
    "\n",
    "This is also extensively covered on SE's various posts like\n",
    "\n",
    "https://math.stackexchange.com/a/4872870/145325\n",
    "\n",
    "https://math.stackexchange.com/a/2260451/145325"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15973462",
   "metadata": {},
   "source": [
    "#### Exercise 7.9.10. \n",
    "\n",
    "Referring to Example $7.9.5$ of this section, determine $c$ so that $$P(−c<T_1 −\\theta<c|T_2 =t_2)=0.95.$$\n",
    "Use this result to find a $95\\%$ confidence interval for $\\theta$, given $T_2 = t_2$; and note how\n",
    "its length is smaller when the range of $t_2$ is larger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82137a9",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "The conditional pdf is given in Example $7.9.5$ as\n",
    "\n",
    "$$h_{1|2}(t_1|t_2;\\theta) = \\frac{1}{2-t_2}, ~ \\theta−1+\\frac{t_2}{2} <t_1 <\\theta+1−\\frac{t_2}{2},~ 0<t_2 <2.$$\n",
    "\n",
    "We just need $c$ such that\n",
    "$$\n",
    "\\begin{align}\n",
    "P(-c < T_1 -\\theta < c) &= 0.95\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "But\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(-c < T_1 -\\theta < c) &= P(\\theta-c < T_1 < \\theta+c) \\\\\n",
    "&= \\frac{(\\theta+c)-(\\theta-c)}{2-t_2} \\\\\n",
    "&= \\frac{2c}{2-t_2} \\\\\n",
    "&= 0.95 \\\\\n",
    "\\implies c &= 0.95\\left(1-\\frac{t_2}{2}\\right).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Following the line of reasoning in Section $4.2$ where they arrive at the confidence interval for the mean based on samples from a normal distribution (refer to equation $4.2.3$), we basically have $P(T_1-c < \\theta < T_1+c) = 0.95$ so that $(t_1-c,t_1+c)$ is our $95\\%$ confidence interval given $T_2 = t_2$, i.e. $95\\%$ confidence interval is given by\n",
    "\n",
    "$$\\left(t_1 - 0.95\\left ( 1-\\frac{t_2}{2}\\right ), t_1+0.95\\left(1-\\frac{t_2}{2}\\right)\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cd2541",
   "metadata": {},
   "source": [
    "#### Exercise 7.9.11. \n",
    "\n",
    "Show that $Y = |X|$ is a complete sufficient statistic for $\\theta > 0$, where $X$ has the pdf $f_X(x;\\theta) = 1/(2\\theta),$ for $−\\theta < x < \\theta,$ zero elsewhere. Show that $Y = |X|$ and $Z = \\text{sgn}(X)$ are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0e1969",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "Strange that in this example, the 'random sample' has a sample size of one!\n",
    "\n",
    "Basically, the pdf is also the likelihood function and can be written as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_X(x;\\theta) &= 1/(2\\theta) I_{(-\\theta,\\theta)}(x) \\\\\n",
    "&= 1/(2\\theta) I_{(0,\\theta)}(|x|)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which has a clear trivial factorization if we define the statistic $Y \\triangleq |X|$ making this statistic a sufficient statistic. $Y$ has $U(0,\\theta)$ distribution.\n",
    "\n",
    "Note that the generic complete sufficient statistic when we have a random sample of size $n>1$ is $\\max\\{|X_i|\\}_{i=1}^n$. Minimal sufficiency has been proved in the solution to exercise $7.8.2.$\n",
    "\n",
    "For completeness, if a certain function $g$ is such that $E_{\\theta}(Y) = 0$, it is kind of trivial to show that $g(Y) = 0$ almost surely (the expectation integral has positive terms forcing the function to be zero almost surely).\n",
    "\n",
    "Now $\\text{sgn}(W)$, where $W$ has $U(-1,1)$ distribution, is scale independent. Hence $Y = |X|$ and $Z = \\text{sgn}(X)$ are independent from theorem $7.9.1$.\n",
    "\n",
    "#### Useful SE posts:\n",
    "\n",
    "https://math.stackexchange.com/a/700088/145325\n",
    "\n",
    "https://math.stackexchange.com/a/2890352/145325"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b77e0f",
   "metadata": {},
   "source": [
    "#### Exercise 7.9.12. \n",
    "\n",
    "Let $Y_1 < Y_2 < \\cdots < Y_n$ be the order statistics of a random sample from a $N(\\theta,\\sigma^2)$ distribution, where $\\sigma^2$ is fixed but arbitrary. Then $Y = \\overline{X}$ is a complete sufficient statistic for $\\theta$. Consider another estimator $T$ of $\\theta$, such as $T = (Y_i + Y_{n+1−i} )/2,$ for $i = 1, 2, \\cdots , [n/2],$ or $T$ could be any weighted average of these latter statistics.\n",
    "\n",
    "**(a)** Argue that $T − \\overline{X}$ and $\\overline{X}$ are independent random variables.\n",
    "\n",
    "**(b)** Show that $Var(T ) = Var(X) + Var(T − X).$\n",
    "\n",
    "**(c)** Since we know $Var(X) = \\sigma^2/n,$ it might be more efficient to estimate $Var(T)$ by estimating the $Var(T −\\overline{X})$ by Monte Carlo methods rather than doing that with $Var(T)$ directly, because $Var(T ) \\geq Var(T − \\overline{X}).$ This is often called the Monte Carlo Swindle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db487666",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "This looks more like a piece of tutorial than being an exercise problem.\n",
    "\n",
    "**(a)** We can write $X_i = \\theta + W_i$ where the random samples $W_i$ are from $N(0,\\sigma^2)$, i.e independent of $\\theta$ (note that $\\sigma^2$ is fixed). This is reflected in the fact that $E(T-\\overline{X}) = 0$, and hence, the statistic $T-\\overline{X}$ is independent of $\\theta$, as it is a location independent statistic i.e. an ancillary statistic. Hence as a consequence of theorem $7.9.1$, $T-\\overline{X}$ is independent of the complete sufficient statistic $\\overline{X}$.\n",
    "\n",
    "**(b)** Well variance of sum is equal to sum of variances when the random variables are independent. Since we know from (a) that $T-\\overline{X}$ is independent of $\\overline{X}$, we can conclude that $Var(T ) = Var(X) + Var(T − X).$\n",
    "\n",
    "**(c)** Nothing to prove here ... this is more of a mini-tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46607abb",
   "metadata": {},
   "source": [
    "#### Exercise 7.9.13\n",
    "\n",
    "Solutions manual has the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494bedd",
   "metadata": {},
   "source": [
    "#### Exercise 7.9.14. \n",
    "\n",
    "The pdf depicted in Figure $7.9.1$ is given by \n",
    "\n",
    "$$f_{m_2}(x) = e^{−x}(1 + m_2^{−1}e^{−x})^{−(m_2+1)},~ −\\infty < x < \\infty, \\tag{7.9.2}$$\n",
    "\n",
    "where $m_2 > 0$ (the pdf graphed is for $m_2 = 0.1$). This is a member of a large family of pdfs, log $F-$family, which are useful in survival (lifetime) analysis; see Chapter $3$ of Hettmansperger and McKean $(2011)$.\n",
    "\n",
    "**(a)** Let $W$ be a random variable with pdf $(7.9.2)$. Show that $W = \\log Y$ , where $Y$ has an $F-$distribution with $2$ and $2m_2$ degrees of freedom.\n",
    "\n",
    "**(b)** Show that the pdf becomes the logistic $(6.1.8)$ if $m_2 = 1$.\n",
    "\n",
    "**(c)** Consider the location model where $X_i =θ+W_i, ~ i=1,...,n,$\n",
    "where $W_1, \\cdots , W_n$ are iid with pdf $(7.9.2)$. Similar to the logistic location model, the order statistics are minimal sufficient for this model. Show, similar to Example $6.1.2$, that the mle of $\\theta$ exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43dabdd",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "A useful SE post is given below\n",
    "\n",
    "https://stats.stackexchange.com/a/125598/183497\n",
    "\n",
    "**(a)** Referring to equation $3.6.6$, which is the expression for the pdf of $F-$distribution, when we substitute $r_1 = 2$ and $r_2 = 2m_2$ in that equation, then we get the pdf of $Y$ as \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f_Y(y) = \\frac{I_{(0,\\infty)}(y)}{\\left( 1 + \\frac{y}{m_2} \\right)^{1+m_2}}.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "There is a modification required here. We need the transformation to be $W = \\log (1/Y)$ rather than $W = \\log Y$. With that change, we can simply invoke the change of variable (equation $1.7.11$, from theorem $1.7.1$) where $w = g(y) = \\log (1/y)$ so that $y = g^{-1}(w) = e^{-w}$. Now the Jacobian $J = |dy/dw| = e^{-w}$ so that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_W(w) &= f_Y(g^{-1}(w))\\left| \\frac{dy}{dw} \\right| \\\\\n",
    "&= \\frac{e^{-w}}{\\left( 1 + \\frac{e^{-w}}{m_2} \\right)^{1+m_2}} I_{(0,\\infty)}(e^{-w}) \\\\\n",
    "&= \\frac{e^{-w}}{\\left( 1 + \\frac{e^{-w}}{m_2} \\right)^{1+m_2}} I_{(-\\infty,\\infty)}(w),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "that matches the pdf in equation $(7.9.2.)$\n",
    "\n",
    "**(b)** Trivial.\n",
    "\n",
    "**(c)** We have to derive the expressions in example $6.1.2$ and as seen below, the exact same argument from that example in the text can be repeated to conclude that mle exists and is unique.\n",
    "\n",
    "The equations $6.1.9$ and $6.1.10$ can be derived for this pdf of log $F-$distribution with the given location parameter $\\theta$ as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "l'(\\theta) = n - \\frac{1+m_2}{m_2}\\sum_{i=1}^n \\left[ \\cfrac{e^{-(x_i-\\theta)}}{1+\\frac{e^{-(x_i-\\theta)}}{m_2}} \\right]\\tag{6.1.9}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Setting this equation to $0$ and rearranging terms results in the equation\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^n \\left[ \\cfrac{e^{-(x_i-\\theta)}}{1+\\frac{e^{-(x_i-\\theta)}}{m_2}}  \\right ]= \\frac{m_2}{1+m_2}n \\tag{6.1.10}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "which nicely match with the corresponding equations in the text for the case of $m_2=1$ given in section $6.1$.\n",
    "\n",
    "Note that the equations have been labelled as equations $6.1.9$ and $6.1.10$ for convenience. The derivative of the left side of equation $(6.1.10)$ simplifies to\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta} \\sum_{i=1}^n \\left[ \\cfrac{e^{-(x_i-\\theta)}}{1+\\frac{e^{-(x_i-\\theta)}}{m_2}}  \\right ]=\\sum_{i=1}^n \\left[ \\cfrac{e^{-(x_i-\\theta)}}{\\left \\{ 1+\\frac{e^{-(x_i-\\theta)}}{m_2}\\right\\}^2}  \\right ] > 0\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "At this point, we can repeat the exact same argument from the text in section $6.1$ quoted from the text below, with minor but crucial modification marked in $\\textbf{bold}$ font.\n",
    "\n",
    "\"Thus the left side of equation $(6.1.10)$ is a strictly increasing function of $\\theta$. Finally, the left side of $(6.1.10)$ approaches $0$ as $\\theta \\to −\\infty$ and approaches $\\pmb{m_2n}$ as $\\theta \\to \\infty$. Thus equation $(6.1.10)$ has a unique solution. Also, the second derivative of $l(\\theta)$ is strictly negative for all $\\theta$; hence, the solution is a maximum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad4556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
