{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c05fc26",
   "metadata": {},
   "source": [
    "The solutions manual also has some solutions. I have tried to solve as many from the rest (i.e. from the ones whose solutions are not in the soluton manual) as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f95d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import pylab as py\n",
    "import scipy.linalg as la\n",
    "import statistics\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "\n",
    "from math import gamma as tma\n",
    "import itertools\n",
    "from scipy.stats import laplace\n",
    "from scipy.stats import logistic\n",
    "from scipy.stats import cauchy\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import weibull_min as weibull\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal as mnorm\n",
    "from scipy.stats import t as studt\n",
    "from scipy.stats import f as fdist\n",
    "from scipy.stats import chisquare as chisq\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import gaussian_kde as gkde\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import math\n",
    "import sympy as sym\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.cbook import boxplot_stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae31e4f",
   "metadata": {},
   "source": [
    "#### Exercise 7.7.1. \n",
    "\n",
    "Let $Y_1 < Y_2 < Y_3$ be the order statistics of a random sample of size $3$ from the distribution with pdf\n",
    "\n",
    "$$f(x;\\theta_1,\\theta_2) = \\begin{cases} \\frac{1}{\\theta_2}\\exp{-\\left( \\frac{x-\\theta_1}{\\theta_2} \\right)}, & \\theta_1 <x<\\infty, −\\infty<\\theta_1 <\\infty, 0<\\theta_2 <\\infty \\\\ 0, & \\text{elsewhere.}\n",
    "\\end{cases}$$\n",
    "\n",
    "Find the joint pdf of $Z_1 = Y_1$, $Z_2 = Y_2$, and $Z_3 = Y_1 + Y_2 + Y_3$. The corresponding transformation maps the space $\\{(y_1,y_2,y_3) : \\theta_1 < y_1 < y_2 < y_3 < \\infty\\}$ onto the space\n",
    "$\\{(z_1,z_2,z_3):\\theta_1 <z_1 <z_2 <(z_3−z_1)/2<\\infty\\}$. Show that $Z_1$ and $Z_3$ are joint sufficient statistics for $\\theta_1$ and $\\theta_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277494c",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "Joint pdf of the random sample $X_1$, $X_2$, and $X_3$ is $$ \\prod_{i=1}^3 f(x_i;\\theta_1,\\theta_2) = \\begin{cases} \\frac{1}{\\theta_2^3}\\exp{-\\left( \\frac{\\sum_{i=1}^3x_i-3\\theta_1}{\\theta_2} \\right)}, & \\theta_1 <x_1,x_2,x_3<\\infty, −\\infty<\\theta_1 <\\infty, 0<\\theta_2 <\\infty, \\\\ 0, & \\text{elsewhere.}\n",
    "\\end{cases} $$\n",
    "\n",
    "Next, we have to plough through the process of finding the joint pdf of $Z_1$ and $Z_3$. From equation $(4.4.1)$ of the text, we know that pdf of order statistics is $$g(y_1,y_2,y_3) = \\begin{cases} 6f(y_1;\\theta_1,\\theta_2)f(y_2;\\theta_1,\\theta_2)f(y_3;\\theta_1,\\theta_2), & \\theta_1 <y_1 < y_2 < y_3<\\infty, −\\infty<\\theta_1 <\\infty, 0<\\theta_2 <\\infty, \\\\ 0, & \\text{elsewhere.}\n",
    "\\end{cases}$$\n",
    "\n",
    "We are now transforming $(Y_1,Y_2,Y_3) \\to (Z_1,Z_2,Z_3)$ with the given rule of $Z_1 = Y_1$, $Z_2 = Y_2$, and $Z_3 = Y_1 + Y_2 + Y_3$. Inverse transformation is $Y_1 = Z_1$, $Y_2 = Z_2$, and $Y_3 = Z_3 - Z_1 - Z_2$. Jacobian is $1$ so joint pdf of this transfmation ends up being $$h(z_1,z_2,z_3) = \\begin{cases} 6f(z_1;\\theta_1,\\theta_2)f(z_2;\\theta_1,\\theta_2)f(z_3-z_1-z_2;\\theta_1,\\theta_2), & \\theta_1 <z_1 < z_2 < (z_3-z_1)/2<\\infty, −\\infty<\\theta_1 <\\infty, 0<\\theta_2 <\\infty, \\\\ 0, & \\text{elsewhere.}\n",
    "\\end{cases}$$\n",
    "\n",
    "As regards to the support, we have $y_1<y_2<y_3 \\implies z_1 < z_2 < z_3-z_1-z_2$ from which we can arrive at the support for $(z_1,z_2,z_3)$ that is shown in the equation. In this support, the joint pdf of $(Z_1,Z_2,Z_3)$ ends up being $$h(z_1,z_2,z_3) = \\frac{6}{\\theta_2^3}\\exp{-\\left( \\frac{z_3-3\\theta_1}{\\theta_2} \\right)}$$ so that the joint pdf of $(Z_1,Z_3)$ can be obtained by integrating out $h(z_1,z_2,z_3)$ w.r.t $z_2$ between the limits of its support namely $z_1 < z_2 < (z_3-z_1)/2$. From that, we get $$h(z_1,z_3) = \\frac{3(z_3-3z_1)}{\\theta_2^3}\\exp{-\\left( \\frac{z_3-3\\theta_1}{\\theta_2} \\right)}$$ where the support is now $\\theta_1 <z_1 < z_3/3 < \\infty$.\n",
    "\n",
    "From all of this, we can arrive at \n",
    "\n",
    "$$\\cfrac{\\prod_{i=1}^3 f(x_i;\\theta_1,\\theta_2)}{h(z_1,z_3)} = \\frac{1}{3\\{x_1+x_2+x_3-3\\min(x_1,x_2,x_3)\\}}.$$\n",
    "\n",
    "So just as in example $(7.7.1)$, this ratio ends up being independent of the parameters and is strictly a function of the realizations of the random sample. Hence $Z_1$ and $Z_3$ are joint sufficient statistics for $\\theta_1$ and $\\theta_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f3c533",
   "metadata": {},
   "source": [
    "#### Exercise 7.7.2\n",
    "\n",
    "The solution is very similar to that of exercise $7.5.8$ whose solution is given in\n",
    "\n",
    "https://math.stackexchange.com/q/4967955/145325\n",
    "\n",
    "Main trick is that the joint sufficient statistics seperate out, and the term inside the integral (while finding the marginal pdf) will contain additional terms also containing the joint sufficient statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd41025",
   "metadata": {},
   "source": [
    "Exercises $7.7.3$ and $7.7.4$ are solved in the solutions manual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3585465b",
   "metadata": {},
   "source": [
    "#### Exercise 7.7.5. \n",
    "\n",
    "In Example $7.7.2$:\n",
    "\n",
    "(a) Find the MVUE of the standard deviation $\\sqrt{\\theta_2}$.\n",
    "\n",
    "(b) Modify the R function bootse1.R so that it returns the estimate in (a) and\n",
    "its bootstrap standard error. Run it on the Bavarian forest data discussed in Example $4.1.3$, where the response is the concentration of sulfur dioxide. Using $3000$ bootstraps, report the estimate and its bootstrap standard error.\n",
    "\n",
    "#### Useful link:\n",
    "\n",
    "https://stats.stackexchange.com/a/353092/183497"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ece94",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "(a) From Theorem $3.6.1$ part (c), we know that $\\frac{n-1}{\\theta_2}Z_2 \\sim \\chi^2(n-1)$. Define the random variable $V = \\frac{n-1}{\\theta_2}Z_2$. We look at $E(\\sqrt{V})$ in the hopes that it ends up being proportional to the standard deviation $\\sigma = \\sqrt{\\theta_2}$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}\\left( \\frac{\\sqrt{n-1}}{\\sigma} \\sqrt{Z_2} \\right) &= \\mathbb{E}(\\sqrt{V})\\\\\n",
    "&= \\int_0^{\\infty} \\sqrt{v} \\cfrac{v^{\\frac{n-1}{2} - 1}e^{-\\frac{v}{2}}}{\\Gamma \\left(\\frac{n-1}{2}\\right)2^{\\frac{n-1}{2}}}dv \\\\\n",
    "&= \\int_0^{\\infty} \\cfrac{v^{\\frac{n}{2} - 1}e^{-\\frac{v}{2}}}{\\Gamma\\left(\\frac{n-1}{2}\\right)2^{\\frac{n-1}{2}}}dv \\\\\n",
    "&= \\cfrac{\\Gamma\\left(\\frac{n}{2}\\right)2^{\\frac{n}{2}}}{\\Gamma\\left(\\frac{n-1}{2}\\right)2^{\\frac{n-1}{2}}} \\int_0^{\\infty} \\cfrac{v^{\\frac{n}{2} - 1}e^{-\\frac{v}{2}}}{\\Gamma\\left(\\frac{n}{2}\\right)2^{\\frac{n}{2}}}dv \\\\\n",
    "&= \\sqrt{2} \\cfrac{\\Gamma\\left(\\frac{n}{2}\\right)}{\\Gamma\\left(\\frac{n-1}{2}\\right)} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left(\\cfrac{\\Gamma\\left(\\frac{n-1}{2}\\right)}{\\Gamma\\left(\\frac{n}{2}\\right)}  \\sqrt{\\frac{n-1}{2}Z_2} \\right) = \\sigma.\n",
    "$$\n",
    "\n",
    "The required MVUE is then $$ \\cfrac{\\Gamma\\left(\\frac{n-1}{2}\\right)}{\\Gamma\\left(\\frac{n}{2}\\right)}  \\sqrt{\\frac{n-1}{2}Z_2} = \\cfrac{\\Gamma\\left(\\frac{n-1}{2}\\right)}{\\Gamma\\left(\\frac{n}{2}\\right)}  \\sqrt{\\frac{\\sum_{i=1}^n (X_i-\\overline{X})^2}{2}} .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7564ade2",
   "metadata": {},
   "source": [
    "#### Answer from the back of the book\n",
    "\n",
    "$7.7.5$ (a) $\\frac{\\Gamma[(n−1)/2]}{\\Gamma[n/2]} \\sqrt{\\frac{n-1}{2}}S$ \n",
    "\n",
    "(b) Download bootse6.R \n",
    "\n",
    "$10.1837$; Our run: $1.156828$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389ce91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b)\n",
    "# The R codes were converted to Python using the online code converter in\n",
    "# https://www.codeconvert.ai/r-to-python-converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0581877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootse1(x, nb=3000):\n",
    "    n = len(x)\n",
    "    coll = []\n",
    "    xb = np.mean(x)\n",
    "    est = tma((n-1)/2)*np.sqrt(np.sum([(xi-xb)**2 for xi in x])/2)/tma(n/2)\n",
    "    for i in range(nb):\n",
    "        xstar = np.random.choice(x, n, replace=True)\n",
    "        thisxb = np.mean(xstar)\n",
    "        sq_err = [(xi-thisxb)**2 for xi in xstar]\n",
    "        thismvue = tma((n-1)/2)*np.sqrt(np.sum(sq_err)/2)/tma(n/2)\n",
    "        coll.append(thismvue)\n",
    "    se1 = np.std(coll,ddof=1)\n",
    "    return [est,se1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a724e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 Index(['Unnamed: 0', 'sulfurdioxide'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sulfurdioxide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sulfurdioxide\n",
       "0           33.4\n",
       "1           38.6\n",
       "2           41.7\n",
       "3           43.9\n",
       "4           44.4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('data/sulfurdio.csv')\n",
    "print(len(data),data.columns)\n",
    "data = data[['sulfurdioxide']].copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908bfcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.1837731676486, 1.17979729551681)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[est,stderr] = bootse1(data['sulfurdioxide'])\n",
    "\n",
    "est,stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e4853a",
   "metadata": {},
   "source": [
    "Exercise $7.7.6$ is solved in the solutions manual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1956e2a",
   "metadata": {},
   "source": [
    "#### Exercise 7.7.7. \n",
    "\n",
    "Let $X_1,X_2,\\cdots,X_n$ be a random sample from $N(\\theta_1,\\theta_2)$.\n",
    "\n",
    "(a) If the constant $b$ is defined by the equation $P (X \\leq b) = p$ where $p$ is specified, find the mle and the MVUE of $b$.\n",
    "\n",
    "(b) Modify the R function bootse1.R so that it returns the MVUE of Part (a) and its bootstrap standard error.\n",
    "\n",
    "(c) Run your function in Part (b) on the data set discussed in Example $7.6.4$ for $p = 0.75$ and $3000$ bootstraps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d64bf3",
   "metadata": {},
   "source": [
    "#### Answer from the back of the book\n",
    "\n",
    "$7.7.7$ (a) K = $(\\Gamma((n − 1)/2)/\\Gamma(n/2)) \\sqrt{(n − 1)/2}$\n",
    "\n",
    "mvue $= \\Phi^{-1}(p)KS + \\overline{x}$\n",
    "\n",
    "(c) $59.727$; Our run $3.291479$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d8a0a4",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "(a) Well if $P (X \\leq b) = p$, then \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P (X \\leq b) &= p \\\\\n",
    "\\implies P \\left(\\frac{X-\\theta_1}{\\sqrt{\\theta_2}} \\leq \\frac{b-\\theta_1}{\\sqrt{\\theta_2}} \\right) &= p \\\\\n",
    "\\implies b &= \\theta_1 +\\sqrt{\\theta_2}\\Phi^{-1}(p)\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\Phi(\\cdot)$ is the cdf of $N(0,1)$. We might as well call $\\theta_1$ as $\\mu$ and $\\sqrt{\\theta_2}$ as $\\sigma$ so that $b = \\mu +\\sigma\\Phi^{-1}(p)$. From example $6.4.1$, we know that the mles \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\mu} &= \\overline{X}, \\\\ \\hat{\\sigma} &= \\sqrt{\\cfrac{\\sum_{i=1}^n(X_i-\\overline{X})^2}{n}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So if $\\pmb{\\theta} = (\\mu,\\sigma)$, and if $g(\\pmb{\\theta}) = \\mu +\\sigma\\Phi^{-1}(p)$, then (based on the comment in the text right before example $6.4.1$ where the author mentions \"Because the second part of the proof of Theorem $6.1.2$ remains true for $\\pmb{\\theta}$ as a vector, $\\hat{\\eta} = g(\\hat{\\pmb{\\theta}})$ is the mle of $\\eta$\") we have the mle for $b$ as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{b} &= g(\\hat{\\pmb{\\theta}}) \\\\\n",
    "&= \\hat{\\mu} +\\hat{\\sigma}\\Phi^{-1}(p) \\\\\n",
    "&= \\overline{X} + \\sqrt{\\cfrac{\\sum_{i=1}^n(X_i-\\overline{X})^2}{n}}\\Phi^{-1}(p).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Similarly, in the text right before example $7.7.3$, the author mentions\n",
    "\n",
    "\"If $T = h(\\pmb{Y})$ for some function $h$ and $E(T) = \\delta$ then $T$ is the unique minimum variance unbiased estimator of $\\delta$.\"\n",
    "\n",
    "We know the MVUE for $\\mu ~~ (=\\theta_1)$ and $\\sigma ~~(= \\sqrt{\\theta_2}$, from exercise $7.7.5$). Let $\\delta = \\theta_1 +\\sqrt{\\theta_2}\\Phi^{-1}(p)$. So if we figure out $T$ such that $\\mathbb{E}(T) = \\delta= \\theta_1 +\\sqrt{\\theta_2}\\Phi^{-1}(p) = \\mu +\\sigma\\Phi^{-1}(p)$, then $T$ would be the MVUE of $b$. Now let\n",
    "\n",
    "$$T = h(\\pmb{Y}) = \\overline{X} + \\cfrac{\\Gamma\\left(\\frac{n-1}{2}\\right)}{\\Gamma\\left(\\frac{n}{2}\\right)}  \\sqrt{\\frac{\\sum_{i=1}^n (X_i-\\overline{X})^2}{2}} \\Phi^{-1}(p),$$ where the joint complete sufficient statistic $\\pmb{Y}$ is as defined in example 7.7.2. As $\\mathbb{E}(T) = \\theta_1 +\\sqrt{\\theta_2}\\Phi^{-1}(p)$, the required MVUE is $T$ given by the equation above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9188f429",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f97a2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootse1_percentile(x, p=0.75, nb=3000):\n",
    "    n = len(x)\n",
    "    coll = []\n",
    "    thisstd = (tma((n-1)/2) / tma(n/2)) * np.sqrt((n-1)/2) * np.std(x, ddof=1)\n",
    "    est = norm.ppf(p) * thisstd + np.mean(x)\n",
    "    \n",
    "    for i in range(nb):\n",
    "        xstar = np.random.choice(x, size=n, replace=True)\n",
    "        thisstd = (tma((n-1)/2) / tma(n/2)) * np.sqrt((n-1)/2) * np.std(xstar, ddof=1)\n",
    "        ests = norm.ppf(p) * thisstd + np.mean(xstar)\n",
    "        coll.append(ests)\n",
    "    \n",
    "    bootse5 = np.std(coll, ddof=1)\n",
    "    return [est, bootse5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f35e94",
   "metadata": {},
   "source": [
    "(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "983e563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.727735208632524 3.26018231774829\n"
     ]
    }
   ],
   "source": [
    "xexmp7p6p4 = [27.5,50.9,71.1,43.1,40.4,44.8,36.6,53.5,65.2,47.7]\n",
    "xexmp7p6p4.extend([75.7,55.4,61.1,39.8,33.4,57.6,47.9,60.7,27.8,65.2])\n",
    "\n",
    "[est,stderr] = bootse1_percentile(xexmp7p6p4)\n",
    "\n",
    "print(est,stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94397be",
   "metadata": {},
   "source": [
    "#### Exercise 7.7.8. \n",
    "\n",
    "In the notation of Example $7.7.3$, show that the mle of $p_jp_l$ is $n^{−2}Y_jY_l$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32aefba",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "Nothing much to say here except to point out that on page $387$ of the text (Section **$6.4,$ Multiparameter Case: Estimation**), authors mention\n",
    "\n",
    "\"Often we are interested in a function of $\\pmb{\\theta}$, say, the parameter $\\eta = g(\\pmb{\\theta})$. Because the second part of the proof of Theorem $6.1.2$ remains true for $\\pmb{\\theta}$ as a vector, $\\hat{\\eta} = g(\\hat{\\pmb{\\theta}})$ is the mle of $\\eta$.\"\n",
    "\n",
    "So just let $g(\\hat{\\pmb{\\theta}} = p_jp_l$. Now $Y_j$ and $Y_l$ are single trial from a Binomial distribution with parameters $(n,p_j)$ and $(n,p_l)$. We know that mle of $p_j$ and $p_l$ are $Y_j/n$ and $Y_l/n$, and hence, the result follows immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e9750",
   "metadata": {},
   "source": [
    "#### Exercise 7.7.9. \n",
    "\n",
    "Refer to Example $7.7.4$ on sufficiency for the multivariate normal model.\n",
    "\n",
    "(a) Determine the MVUE of the covariance parameters $\\sigma_{ij}$ .\n",
    "\n",
    "(b) Let $g = \\sum_{i=1}^k a_i\\mu_i$, where $a_1, \\cdots, a_k$ are specified constants. Find the MVUE for $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841be780",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "Solutions manual has the solution to part (a). \n",
    "\n",
    "(b) MVUE for $\\mu_j$ is $\\overline{X_j}$ so MVUE for $g$ is $\\sum_{j=1}^k a_jX_j$. Here, $\\overline{X_j}=n^{-1}\\sum_{i=1}^n X_{ij}$ as defined in the text (page $452$ in the paragraphs after equation $7.7.8$).\n",
    "\n",
    "This is based on what the authors have mentioned in the text (page $448$). Quoting them below:\n",
    "\n",
    "\"The Rao–Blackwell, Lehmann–Scheffe theory outlined in Sections $7.3$ and $7.4$\n",
    "extends naturally to this vector case. Briefly, suppose $\\delta = g(\\pmb{\\theta})$ is the parameter of interest and $\\pmb{Y}$ is a vector of sufficient and complete statistics for $\\pmb{\\theta}$. Let $T$ be a statistic that is a function of $\\pmb{Y}$, such as $T = T(\\pmb{Y})$. If $E(T) = \\delta$, then $T$ is the unique MVUE of $\\delta$.\"\n",
    "\n",
    "So in this case, $T=g$ and $\\pmb{\\theta} = (\\mu_1,\\mu_2,\\cdots,\\mu_k)$ and $\\pmb{Y} = (\\overline{X_1},\\overline{X_2},\\cdots,\\overline{X_k})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d374b6ea",
   "metadata": {},
   "source": [
    "#### Exercise 7.7.10. \n",
    "\n",
    "In a personal communication, LeRoy Folks noted that the inverse Gaussian pdf\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f(x;\\theta_1,\\theta_2) = \\left(\\cfrac{\\theta_2}{2\\pi x^3}\\right)^{1/2}\\exp\\left[ \\frac{-\\theta_2(x-\\theta_1)^2}{2\\theta_1^2 x} \\right], ~~ 0<x<\\infty, \\tag{7.7.9}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\theta_1 > 0$ and $\\theta_2 > 0$, is often used to model lifetimes. Find the complete sufficient statistics for $(\\theta_1,\\theta_2)$ if $X_1,X_2,\\cdots,X_n$ is a random sample from the distribution having this pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fbb061",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\prod_{i=1}^n f(x_i;\\theta_1,\\theta_2) = \\exp{\\left[ -\\sum_{i=1}^n\\left( \\frac{\\theta_2x_i}{2\\theta_1^2} + \\frac{\\theta_2}{2x_i} \\right) + n\\left( \\frac{\\theta_2}{\\theta_1}+\\frac{1}{2}\\log{\\frac{\\theta_2}{2\\theta_1^2}} \\right) +\\sum_{i=1}^n\\frac{1}{2}\\log{\\frac{1}{x_i}} \\right]}, ~~ 0<x_i<\\infty, ~~ i\\in\\{1,2,\\cdots,n\\},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "so following the material after equation $7.7.3$ of the text where the authors describe how to choose the complete sufficient statistics for a given exponential family of distributions, the complete sufficient statistics in this case are \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Y_1 &= \\sum_{i=1}^nX_i \\\\\n",
    "Y_2 &= \\sum_{i=1}^n\\frac{1}{X_i}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which also matches the answer given at the end of the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5912de41",
   "metadata": {},
   "source": [
    "#### Exercise 7.7.11.\n",
    "\n",
    "Let $X_1,X_2,\\cdots,X_n$ be a random sample from a $N(\\theta_1,\\theta_2)$ distribution. \n",
    "\n",
    "(a) Show that $E[(X_1 − \\theta_1)^4] = 3\\theta_2^2.$\n",
    "\n",
    "(b) Find the MVUE of $3\\theta_2^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b69863",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "The joint complete sufficient statistics for $N(\\theta_1,\\theta_2)$ are $(\\overline{X},S^2)$, the sample mean and sample variance. Similar to what was done in\n",
    "\n",
    "https://stats.stackexchange.com/a/353092/183497\n",
    "\n",
    "or \n",
    "\n",
    "https://math.stackexchange.com/a/3082982/145325,\n",
    "\n",
    "it is pretty trivial to show (for part (b)) that\n",
    "\n",
    "$$T = \\cfrac{3(n-1)^2\\Gamma\\left(\\frac{n-1}{2}\\right)}{4\\Gamma\\left(\\frac{n+3}{2}\\right)}S^4,$$ where $S^2=\\frac{\\sum_{i=1}^n(X_i-\\overline{X})^2}{n-1},$ the sample variance of the given random samples. For part (a), it is well known as to how we can find central moments using the mgf technique (see for instance https://math.stackexchange.com/a/92650/145325) and the conclusion is immediate when we use the general formula for the central moments of normal distribution.\n",
    "\n",
    "Not sure how (a) related to (b). Someone on SE (link -- https://math.stackexchange.com/a/4698982/145325) has shown that\n",
    "\n",
    "$$T' = \\frac{n}{3\\left(n-1\\right)^2} \\sum_{i=1}^n \\left(X_i - \\overline X\\right)^4$$ is an unbiased estimator of $\\sigma^4$. But this is not the MVUE. So the fact that $\\mathbb{E}[(X_1-\\theta_1)^4]=3\\theta_2^2$ does not seem to be related to finding the MVUE as the form of MVUE is surely not same as that of $T'$ shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13b0192",
   "metadata": {},
   "source": [
    "#### Exercise 7.7.12. \n",
    "\n",
    "Let $X_1,\\cdots,X_n$ be a random sample from a distribution of the continuous type with cdf $F(x)$. Suppose the mean, $\\mu = E(X_1)$, exists. Using Example $7.7.5$, show that the sample mean, $X = n^{-1}\\sum_{i=1}^nX_i$, is the MVUE of $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab6e575",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "One line answer to exercise $7.7.12$ is given in the solutions manual. I'm adding some of my own comments to record the argument in detail. The part of the text most relevant to this and the next exercise is the following (page $452$ of the text right at the end of example $7.7.5$)\n",
    "\n",
    "> Let $T = T(x_1,x_2,\\cdots,x_n)$ be any statistic that is symmetric in its arguments; i.e., $T(x_1,x_2,...,x_n) = T(x_{i_1},x_{i_2},...,x_{i_n})$ for any permutation $(x_{i_1},x_{i_2},...,x_{i_n})$ of $(x_1, x_2, . . . , x_n)$. Then $T$ is a function of the order statistics.\n",
    "\n",
    "So for instance, in exercise $7.7.12$, $T = \\overline{X}=n^{-1}\\sum_{i=1}^nX_i = n^{-1}\\sum_{i=1}^nY_i$, i.e. it is a function of the order statistics as expected as permutation keeps the mean invariant. The order statistics is complete sufficient statistics for the \"family of cdfs\" (see https://stats.stackexchange.com/q/656151/183497) and as $\\mathbb{E}(\\overline{X}) = \\mu$ i.e as $\\overline{X}$ is unbiased for the mean (which is known to exists), it is also the MVUE of the mean $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea76c7f3",
   "metadata": {},
   "source": [
    "#### Exercise 7.7.13. \n",
    "\n",
    "Let $X_1,\\cdots,X_n$ be a random sample from a distribution of the continuous type with cdf $F(x)$. Let $θ = P(X_1 \\leq a) = F(a)$, where $a$ is known. Show that the proportion $n^{-1}\\#\\{X_i \\leq a\\}$ is the MVUE of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83433b35",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "Let's make the statistic a bit more formal. As defined in example $7.6.3$, let\n",
    "\n",
    "$$\n",
    "u_a(x) = \\begin{cases}\n",
    "1, & x\\leq a \\\\\n",
    "0, & x > a.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then the proportion statistic $T = n^{-1}\\#\\{X_i \\leq a\\}$ can be written in terms of this function as $T = n^{-1}\\sum_{i=1}^n u_a(X_i) = n^{-1}\\sum_{i=1}^n u_a(Y_i),$ a function of the order statistics which are complete and sufficient statistics for $F(x)$. \n",
    "\n",
    "Also, as shown in example $7.6.3$ (they have normal distribution in that example), $\\mathbb{E}[u_a(X_i)] = F(a), ~ \\forall i \\in \\{1,2,\\cdots , n \\},$ so that $\\mathbb{E}(T)=F(a).$ Hence $T$ is unbiased for $F(a)$. As $T$ is a function of the order statistics that are complete sufficient statistics for $F(x)$, we can conclude that it is the MVUE for $F(a)$ assuming $a$ is known."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b7b958",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
