{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c05fc26",
   "metadata": {},
   "source": [
    "The solutions manual also has some solutions. I have tried to solve as many from the rest (i.e. from the ones whose solutions are not in the soluton manual) as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45f95d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import pylab as py\n",
    "import scipy.linalg as la\n",
    "import statistics\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "\n",
    "from math import gamma as tma\n",
    "import itertools\n",
    "from scipy.stats import laplace\n",
    "from scipy.stats import logistic\n",
    "from scipy.stats import cauchy\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import weibull_min as weibull\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal as mnorm\n",
    "from scipy.stats import t as studt\n",
    "from scipy.stats import f as fdist\n",
    "from scipy.stats import chisquare as chisq\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import gaussian_kde as gkde\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import math\n",
    "import sympy as sym\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.cbook import boxplot_stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7415ce23",
   "metadata": {},
   "source": [
    "#### Exercise 7.8.1. \n",
    "\n",
    "Let $X_1, X_2,\\cdots, X_n$ be a random sample from each of the following distributions involving the parameter $\\theta$. In each case find the mle of $\\theta$ and show that it is a sufficient statistic for $\\theta$ and hence a minimal sufficient statistic.\n",
    "\n",
    "**(a)** $b(1,\\theta)$,where $0 \\leq \\theta \\leq 1$. \n",
    "\n",
    "**(b)** Poisson with mean $\\theta > 0$.\n",
    "\n",
    "**(c)** Gamma with $\\alpha=3$ and $\\beta = \\theta > 0$. \n",
    "\n",
    "**(d)** $N(θ,1)$, where $−\\infty < \\theta < \\infty$.\n",
    "\n",
    "**(e)** $N(0,θ)$,where $0 < \\theta < \\infty$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e89920f",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "Note that any one-to-one transformation of a sufficient statistic is also sufficient (like a constant times a sufficient statistic is also sufficient). See\n",
    "\n",
    "https://math.stackexchange.com/q/1256636/145325\n",
    "\n",
    "https://math.stackexchange.com/a/3540689/145325\n",
    "\n",
    "Also, in the beginning of Section $7.3$, authors have mentioned\n",
    "\n",
    "\"First note that a sufficient estimate is not unique in any sense. For if $Y_1 = u_1(X_1, X_2,\\cdots, X_n)$ is a sufficient statistic and $Y_2 = g(Y_1)$ is a statistic, where **$\\bf g(x)$ is a one-to-one function**, then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x_1;\\theta)f(x_2;\\theta)···f(x_n;\\theta) &= k_1[u_1(y_1);\\theta]k_2(x_1,x_2,\\cdots,x_n) \\\\ &= k_1[u_1(g^{−1}(y_2)); \\theta]k_2(x_1, x_2, \\cdots , x_n);\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "hence, by the factorization theorem, Y2 is also sufficient.\"\n",
    "\n",
    "For cases **(a)** through **(d)**, mle $\\hat{\\theta}$ is proportional to $(\\hat{\\theta} \\propto) \\overline{X}$ which is a sufficient statistic for those cases and hence a minimal sufficient statistic.\n",
    "\n",
    "For case **(d)**, mle is the sample variance $\\hat{\\theta} \\propto S^2$. As shown in exercise $7.2.1$, the sample variance is a sufficient statistic for this case and hence a minimal sufficient statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d9effe",
   "metadata": {},
   "source": [
    "#### Exercise 7.8.2.\n",
    "\n",
    "Let $Y_1 < Y_2 < \\cdots < Y_n$ be the order statistics of a random sample of size $n$ from the uniform distribution over the closed interval $[−\\theta,\\theta]$ having pdf $f (x; \\theta) = (1/2\\theta)I_{[−\\theta,\\theta]}(x).$\n",
    "\n",
    "**(a)** Show that $Y_1$ and $Y_n$ are joint sufficient statistics for $\\theta$.\n",
    "\n",
    "**(b)** Argue that the mle of $\\theta$ is $\\hat{\\theta} = \\max(−Y_1, Y_n).$\n",
    "\n",
    "**(c)** Demonstrate that the mle $\\hat{\\theta}$ is a sufficient statistic for $\\theta$ and thus is a minimal sufficient statistic for $\\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa001732",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "\n",
    "Solutions manual has the solution. The following SE posts also have solutions to most parts of this question.\n",
    "\n",
    "https://stats.stackexchange.com/a/354941/183497\n",
    "\n",
    "https://stats.stackexchange.com/a/391857/183497\n",
    "\n",
    "https://math.stackexchange.com/a/2795653/145325\n",
    "\n",
    "However, I thought of adding the solution I came up with. Note that this situation is same as that in example $7.7.1$ with $\\theta_1 = 0$ and $\\theta_2 = \\theta$.\n",
    "\n",
    "**(a)** Direct application of equation $4.4.3$ gives us\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g_{1,n}(y_1,y_n) &= \\frac{n(n-1)}{(2\\theta)^n}(y_n-y_1)^{n-2}, ~~ -\\theta \\leq y_1 < y_n \\leq \\theta \\\\\n",
    "&= \\frac{n(n-1)}{(2\\theta)^n}(y_n-y_1)^{n-2} I_{[−\\theta,\\theta]}(y_1) I_{[−\\theta,\\theta]}(y_n).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Also, the joint pdf of the the random sample is the product of their individual pdfs, i.e\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\prod_{i=1}^n f(x_i,\\theta) = \\prod_{i=1}^n \\frac{1}{(2\\theta)} I_{[−\\theta,\\theta]}(x_i).\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Notice that $\\prod_{i=1}^n I_{[−\\theta,\\theta]}(x_i) = I_{[−\\theta,\\theta]}(y_1) I_{[−\\theta,\\theta]}(y_n)$. Hence we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\cfrac{\\prod_{i=1}^n f(x_i,\\theta)}{g_{1,n}(y_1,y_n)} &= \\prod_{i=1}^n \\frac{1}{(2\\theta)} I_{[−\\theta,\\theta]}(x_i) \\div \\left( \\frac{n(n-1)}{(2\\theta)^n}(y_n-y_1)^{n-2} I_{[−\\theta,\\theta]}(y_1) I_{[−\\theta,\\theta]}(y_n) \\right) \\\\ \n",
    "&=  \\frac{1}{(2\\theta)^n}  I_{[−\\theta,\\theta]}(y_1) I_{[−\\theta,\\theta]}(y_n) \\div \\left( \\frac{n(n-1)}{(2\\theta)^n}(y_n-y_1)^{n-2} I_{[−\\theta,\\theta]}(y_1) I_{[−\\theta,\\theta]}(y_n) \\right) \\\\\n",
    "&= \\frac{1}{n(n-1)(y_n-y_1)^{n-2}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The ratio is free of $\\theta$ and hence $(Y_1,Y_n)$ are joint sufficient statistics for $\\theta$.\n",
    "\n",
    "**(b)** The likelihood function is given in part **(a)** and is \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\prod_{i=1}^n f(x_i,\\theta) = \\prod_{i=1}^n \\frac{1}{(2\\theta)} I_{[−\\theta,\\theta]}(x_i)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "and as we know that $\\prod_{i=1}^n I_{[−\\theta,\\theta]}(x_i) = I_{[−\\theta,\\theta]}(y_1) I_{[−\\theta,\\theta]}(y_n)$, the likelihood function turns out to be\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\prod_{i=1}^n f(x_i,\\theta) &= \\frac{1}{(2\\theta)^n} I_{[−\\theta,\\theta]}(y_1) I_{[−\\theta,\\theta]}(y_n) \\\\\n",
    "&= \\frac{1}{(2\\theta)^n} I_{[0,\\theta]}(\\hat{\\theta}),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\hat{\\theta} = \\max(−y_1, y_n).$ We can show this by taking the three cases.\n",
    "\n",
    "In the first case, all samples are positive. Then as $0 < y_1 < y_n$, it is easy to see that $I_{[−\\theta,\\theta]}(y_1) I_{[−\\theta,\\theta]}(y_n) = I_{[0,\\theta]}(y_n)$. Also, $\\hat{\\theta} = \\max(−y_1, y_n) = y_n$ in this case and hence, $I_{[−\\theta,\\theta]}(y_1) I_{[−\\theta,\\theta]}(y_n)= I_{[0,\\theta]}(y_n) = I_{[0,\\theta]}(\\hat{\\theta}).$\n",
    "\n",
    "In the second case, all samples are negative. Then as $y_1 < y_n < 0$, it is easy to see that $I_{[−\\theta,\\theta]}(y_1) I_{[−\\theta,\\theta]}(y_n) = I_{[0,\\theta]}(-y_1)$. Also, $\\hat{\\theta} = \\max(−y_1, y_n) = -y_1$ in this case and hence, $I_{[−\\theta,\\theta]}(y_1) I_{[−\\theta,\\theta]}(y_n)= I_{[0,\\theta]}(-y_1) = I_{[0,\\theta]}(\\hat{\\theta}).$\n",
    "\n",
    "The third case is when we have a mixture of both positive and negative terms. In that case, one of $-y_1$ and $y_n$ will be larger. If we know that that larger of the two is less than $\\theta$, then the other is less as well. Put in other words, the larger of $-y_1$ and $y_n$ will decide if the product is zero or one. Hence even in this case, $I_{[−\\theta,\\theta]}(y_1) I_{[−\\theta,\\theta]}(y_n)= I_{[0,\\theta]}(\\hat{\\theta}).$ \n",
    "\n",
    "But lower $\\theta$ is, higher will be the value of the likelihood function. So we need it to be as low as possible. Hence the mle would be $\\hat{\\theta} = \\max(-y_1,y_n)$.\n",
    "\n",
    "**(c)** We have pretty much demonstrated in part **(b)** that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\prod_{i=1}^n f(x_i,\\theta) &= \\frac{1}{(2\\theta)^n} I_{[−\\theta,\\theta]}(y_1) I_{[−\\theta,\\theta]}(y_n) \\\\\n",
    "&= \\left [ \\frac{1}{(2\\theta)^n} I_{[0,\\theta]}(\\hat{\\theta}) \\right ] \\times 1,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Hence the likelihood function turns out nicely as a function of the mle $\\hat{\\theta}$, and unity (which is independent of $\\theta$). Hence by factorization theorem, mle is sufficient.\n",
    "\n",
    "One point to note is that $$\\prod_{i=1}^n I_{[−\\theta,\\theta]}(x_i) = I_{[−\\theta,\\theta]}(y_1) I_{[−\\theta,\\theta]}(y_n) = I_{[0,\\theta]}(\\hat{\\theta}),$$ but $$\\prod_{i=1}^n I_{[−\\theta,\\theta]}(x_i) = \\prod_{i=1}^n I_{[0,\\theta]}(|x_i|) = I_{[0,\\theta]}(\\max \\{ |x_i| \\}_{i=1}^n),$$ so that $$ \\hat{\\theta} = \\max(-y_1,y_n) =  \\max \\{ |x_i| \\}_{i=1}^n,$$ is an alternative expression for the mle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8fd7a5",
   "metadata": {},
   "source": [
    "#### Exercise 7.8.3\n",
    "\n",
    "Let $Y_1 < Y_2 < \\cdots < Y_n$ be the order statistics of a random sample of size $n$ from a distribution with pdf\n",
    "\n",
    "$$ f(x;\\theta_1,\\theta_2) = \\frac{1}{\\theta_2} e^{-(x-\\theta_1)/\\theta_2}I_{(\\theta_1,\\infty)}(x),$$ \n",
    "\n",
    "where $ −\\infty < \\theta_1 < \\infty, $ and $0 < \\theta_2 < \\infty.$ Find the joint minimal sufficient statistics\n",
    "for $\\theta_1$ and $\\theta_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b612428",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "There are many SE posts on this topic. See for example\n",
    "\n",
    "https://stats.stackexchange.com/a/506724/183497\n",
    "\n",
    "https://math.stackexchange.com/a/2438489/145325\n",
    "\n",
    "https://stats.stackexchange.com/a/600613/183497\n",
    "\n",
    "https://math.stackexchange.com/a/3740796/145325\n",
    "\n",
    "https://math.stackexchange.com/a/4110064/145325\n",
    "\n",
    "FWIW, I am adding my own version adding to the insane level of redundancy.\n",
    "\n",
    "The likelihood function is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\prod_{i=1}^n f(x_i;\\theta_1,\\theta_2) &= \\prod_{i=1}^n \\frac{1}{\\theta_2} e^{-(x_i-\\theta_1)/\\theta_2}I_{(\\theta_1,\\infty)}(x_i) \\\\\n",
    "&= \\frac{1}{\\theta_2^n}e^{-n(\\overline{x}-\\theta_1)/\\theta_2}I_{(\\theta_1,\\infty)}(y_1),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the second equality uses the fact that $\\prod_{i=1}^n I_{(\\theta_1,\\infty)}(x_i) = I_{(\\theta_1,\\infty)}(y_1)$ (if the least sample is greater than $\\theta_1$, so are the rest of the samples).\n",
    "\n",
    "For a given $\\theta_2$, the likelihood is maximized when $\\theta_1$ is as high as possible. The highest it can be is $\\theta_1=y_1$ as otherwise the indicator function will turn zero. So $\\hat{\\theta_1}=y_1$. We can differentiate w.r.t $\\theta_2$ to get the mle and it turns out to be $\\hat{\\theta_2}=\\overline{x}-y_1$, the difference of sample mean and the mle for $\\theta_1$.\n",
    "\n",
    "We just need to show that $(y_1,\\overline{x}-y_1)$ are sufficient for $(\\theta_1,\\theta_2)$. The starting point is again the likelihood function. All we need to be able to do is to express it as a function of $(y_1,\\overline{x}-y_1)$ and we'll be done (because likelihood function seperates into product of functions of the sufficient statistics and that of the random samples independent of parameters).\n",
    "\n",
    "Let $z=\\overline{x}-y_1$. Then the likelihood function is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\prod_{i=1}^n f(x_i;\\theta_1,\\theta_2) &= \\frac{1}{\\theta_2^n}e^{-n(\\overline{x}-\\theta_1)/\\theta_2}I_{(\\theta_1,\\infty)}(y_1) \\\\\n",
    "&= \\frac{1}{\\theta_2^n}e^{-nz/\\theta_2} e^{-n(y_1-\\theta_1)/\\theta_2} I_{(\\theta_1,\\infty)}(y_1),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is basically a function of the sufficient statistics and the parameters and does not involve the random sample realizations at all. So the likelihood function trivially satisfies the hypothesis for the separation theorem and we can conclude that $(y_1,\\overline{x}-y_1)$ are joint minimal sufficient statistics for $(\\theta_1,\\theta_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4658e522",
   "metadata": {},
   "source": [
    "Exercises $7.8.4$, $7.8.5$, and $7.8.6$ are too trivial and are not worth the time. \n",
    "\n",
    "Solution for exercise $7.8.7$ is given in the solutions manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c52234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
